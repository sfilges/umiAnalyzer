#' Add UMI sample to an existing experiment object
#'
#'
#' @param object UMIexperiment object
#' @param sampleName Name of new sample
#' @param sampleDir Directory to new sample
#' @param clearData Should other data in UMIexperiment be cleared
#'
#' @importFrom dplyr bind_rows
#'
#' @export
#'
addUmiSample <- function(
  object,
  sampleName,
  sampleDir,
  clearData = FALSE) {

  if(missing(x = object)) {
    stop("No UMIexperiment object supplied.")
  } else if(!class(object) == "UMIexperiment"){
    stop("Object is not of class UMIexperiment.")
  } else if (!dir.exists(sampleDir)){
    stop("No valid path provided.")
  } else if(!is.logical(clearData)) {
    warning("clearData needs to be of type boolean. Using defaults instead.")
    clearData = FALSE
  }

  newSample <- createUmiSample(
    sampleName = sampleName,
    sampleDir = sampleDir,
    importBam = FALSE
  )

  newConsData <- newSample@cons.data
  newConsData$sample <- sampleName

  object@cons.data <- dplyr::bind_rows(object@cons.data, newConsData)
}

#' createUmiSample
#'
#' Method for creating a UMI sample from umierrorcorrect output.
#'
#' @param sampleName UMI sample object name
#' @param sampleDir Path to UMI sample folders. Must be a folder generated by umierrorcorrect
#' @param importBam Logical. Should bam files be imported at object initilization? Default is False.
#'
#' @export
#'
#' @import tibble
#' @importFrom readr read_delim cols col_character col_double
#' @importFrom methods new
#' @importFrom utils read.csv
#' @importFrom dplyr rename
#'
createUmiSample <- function(
  sampleName,
  sampleDir,
  importBam = FALSE
  ) {

  if(!dir.exists(sampleDir)){
    warning("You need to provide a valid path.")
    return(NULL)
  } else if(!is.logical(importBam)){
    warning("importBam needs to be of type boolean. Using defaults instead.")
    importbam = FALSE
  }

  consFile <- list.files(path = sampleDir, pattern = "\\.cons$")
  summaryFile <- list.files(path = sampleDir, pattern = "\\_summary_statistics.txt$")

  if(length(consFile) == 0 | length(summaryFile) == 0){
    warning(
      paste(
        "No consensus or summary file found for sample: ",
        sampleName,". Did you supply a correct data folder?"
        )
      )
    return(NULL)
  }

  consTable <- readr::read_delim(
    file = file.path(sampleDir, consFile),
    delim = "\t",
    col_types = readr::cols(
      `Sample Name` = readr::col_character(),
      Contig = readr::col_character(),
      Position = readr::col_double(),
      Name = readr::col_character(),
      Reference = readr::col_character(),
      A = readr::col_double(),
      C = readr::col_double(),
      G = readr::col_double(),
      T = readr::col_double(),
      I = readr::col_double(),
      D = readr::col_double(),
      N = readr::col_double(),
      Coverage = readr::col_double(),
      `Consensus group size` = readr::col_double(),
      `Max Non-ref Allele Count` = readr::col_double(),
      `Max Non-ref Allele Frequency` = readr::col_double(),
      `Max Non-ref Allele` = readr::col_character()
    )
  )

  summaryTable <- readr::read_delim(
    file = file.path(sampleDir, summaryFile),
    delim = "\t",
    col_names = FALSE,
    col_types = readr::cols(
      X1 = readr::col_character(),
      X2 = readr::col_character(),
      X3 = readr::col_character(),
      X4 = readr::col_double(),
      X5 = readr::col_double(),
      X6 = readr::col_double(),
      X7 = readr::col_double()
    )
  ) %>%
    dplyr::rename(
      ID = .data$X1,
      region = .data$X2,
      assay = .data$X3,
      depth = .data$X4,
      fraction = .data$X5,
      totalCount = .data$X6,
      UMIcount = .data$X7
    )

  if (importBam) {
    readsTable <- readBamFile(sampleDir = sampleDir)

    UMIsample <- UMIsample(
      name = sampleName,
      cons.data = consTable,
      summary.data = summaryTable,
      reads = readsTable
    )

    return(UMIsample)
  }
  else {
    UMIsample <- UMIsample(
      name = sampleName,
      cons.data = consTable,
      summary.data = summaryTable,
      reads = tibble()
    )

    return(UMIsample)
  }
}

#' Method for creating a UMI experiment object
#'
#'
#' @param experimentName Name of the experiment
#' @param mainDir Main experiment directory
#' @param sampleNames List of sample names. Can be either NULL or list. If NULL all subdirectories of mainDir will be searched.
#' @param importBam Logical. Should bam files be imported on creation? Default is False.
#' @param as.shiny Set to TRUE if run within a shiny::withProgress environment
#'
#' @export
#'
#' @import tibble
#' @importFrom stringr str_remove
#' @importFrom utils read.csv
#' @importFrom methods new
#' @importFrom dplyr bind_rows
#' @importFrom shiny incProgress
#'
#'
createUmiExperiment <- function(
  mainDir,
  experimentName = NULL,
  sampleNames = NULL,
  importBam = FALSE,
  as.shiny = FALSE){

  if (!dir.exists(mainDir)) {
    warning("Must provide a valid directory.")
    return(NULL)
  } else if(!is.null(experimentName)) {
    if( !is.character(experimentName) && length(experimentName) == 1 ) {
      warning("experimentName needs to be a string or NULL. Using default.")
      experimentName = NULL
    }
  } else if(!is.logical(importBam)){
    warning("importBam needs to be of type boolean. Using default.")
    importBam = FALSE
  } else if (!is.null(sampleNames) && !is.list(sampleNames)){
    warning("sampleNames must be NULL or a list. Using default.")
    sampleNames = NULL
  }

  # Get sample names
  if (is.null(sampleNames)){
    sampleNames <- list.dirs(
      path = mainDir,
      full.names = FALSE,
      recursive = FALSE
    )
  }

  # Initialise dataframes
  cons.data.merged <- tibble()
  summary.data.merged <- tibble()
  reads.merged <- tibble()

  for (i in 1:length(sampleNames)) {

    if(as.shiny){
      n <- length(sampleNames)
      shiny::incProgress(1/n, detail = paste("Loading sample", i, " of ",n))
    }

    if(!dir.exists(file.path(mainDir, sampleNames[i]))){
      warning("Sample directory not found. Did you provide a top level directory
           containing you sample folders?")
    }

    # Find .cons file
    consFile <- list.files(
      path = file.path(mainDir, sampleNames[i]),
      pattern = "\\.cons$")

    # Remove file ending
    consFile <- stringr::str_remove(consFile, '.cons')

    # Create UMI sample
    sample <- createUmiSample(
      sampleName = consFile,
      sampleDir= file.path(mainDir, sampleNames[i]),
      importBam = importBam
    )

    # Sample returns NULL if no consensus file is present, skip that sample
    if(is.null(sample)){
      next
    }

    cons <- sample@cons.data
    cons$sample <- consFile
    cons.data.merged <- dplyr::bind_rows(cons.data.merged, cons)

    summary <- sample@summary.data
    summary$sample <- consFile
    summary.data.merged <- dplyr::bind_rows(summary.data.merged, summary)

    if(importBam) {
      reads <- sample@reads
      reads$sample <- consFile
      reads.merged <- dplyr::bind_rows(reads.merged, reads)
    }
  }

  # Save experiment object
  UMIexperiment <- UMIexperiment(
    name = experimentName,
    cons.data = cons.data.merged,
    summary.data = summary.data.merged,
    reads = reads.merged
  )

  return(UMIexperiment)
}

#' readBamFile
#'
#' Method for reading bam files using scanBam from the Rsamtools package.
#'
#' @param sampleDir Path to UMI sample
#' @param consDepth Only retain consensus reads of at least cons.depth. Default is 0.
#'
#' @import tibble
#' @import magrittr
#' @importFrom Rsamtools scanBam
#' @importFrom tidyr separate unite
#' @importFrom dplyr filter
#'
readBamFile <- function(
  sampleDir,
  consDepth = 0
  ) {

  if(!dir.exists(sampleDir)){
    stop("You must supply a valid path.")
  }

  bam.file <- list.files(
    path = sampleDir,
    pattern = "\\_consensus_reads.bam$"
  )

  # If no bam files are located print an error message and return NULL
  if(length(bam.file) == 0){
    warning(paste("The directory you supplied does not seem to contain bam files: "),
            sampleDir, sep = '')
    return(NULL)
  } else {
    # Load bam file
    bam <- Rsamtools::scanBam(
      file = file.path(sampleDir, bam.file[1])
    )

    # Extract sequence information from bam object
    sequences <- tibble(
      qname = bam[[1]]$qname,
      chrom = bam[[1]]$rname,
      pos = bam[[1]]$pos,
      seq = as.data.frame(bam[[1]]$seq)$x
    )

    sequences <- tidyr::separate(
      sequences,
      col = .data$qname,
      into = c(NA, NA, NA, 'barcode', 'count'),
      sep = '_',
      remove = TRUE
    ) %>%
      tidyr::separate(
        col = .data$count,
        sep = '=',
        into = c(NA, 'count')
      ) %>%
      tidyr::unite(
        col = 'position',
        .data$chrom,
        .data$pos,
        sep = ':'
      )

    sequences$count %<>% as.integer

    sequences <- dplyr::filter(sequences, count >= consDepth)

    return(sequences)
  }
}

#' Save consensus data
#' If save is set to TRUE data will be written to a csv file otherwise consensu data will
#' be returned as a tibble.
#' @export
#' @importFrom readr write_excel_csv write_delim
#' @param object umiExperimet object
#' @param outDir output directory, defaults to working directory
#' @param save Logical. Should data be saved to file? Default is FALSE.
#' @param delim Single character string, either ';' or ',' or tab
#' @param fileName String. Name of the file to be saved. Default is 'consensus_data.csv'
#'
saveConsData <- function(
  object,
  save = FALSE,
  fileName = 'consensus_data.csv',
  outDir = getwd(),
  delim = ';'
  ){

  if(missing(x = object)) {
    stop("No UMIexperiment object supplied.")
  } else if(!class(object) == "UMIexperiment"){
    stop("Object is not of class UMIexperiment.")
  } else if(!is.logical(save)){
    stop("Save needs to be of type boolean.")
  } else if(!(is.character(fileName) && length(fileName)==1)){
    stop("Invalid file name.")
  } else if(!dir.exists(outDir)) {
    stop("Output directory does not exist.")
  } else if(! delim %in% c(';',',','\t')){
    stop("Invalid delimeter, needs to be comma, semicolon or tab.")
  }

  consData <- object@cons.data

  if (save) {
    path <- file.path(outDir,fileName)
    if (delim == ';') {
      readr::write_excel_csv(consData, path, delim = ';')
    } else if (delim == ',') {
      readr::write_excel_csv(consData, path)
    } else if (delim == '\t') {
      readr::write_delim(consData, path, delim = delim)
    }
  } else {
    return(consData)
  }
}

#' Function to parse bam files
#' @export
#' @import tibble
#' @importFrom dplyr bind_rows progress_estimated
#' @importFrom graphics plot
#' @param mainDir Directory containing umierrorcorrect output folders.
#' @param sampleNames A list of sample names.
#' @param consDepth Only retain consensus reads of at least cons.depth. Default is 0.
#' @param as.shiny Set to TRUE if run within a shiny::withProgress environment
#'
parseBamFiles <- function(
  mainDir,
  sampleNames = NULL,
  consDepth = 0,
  as.shiny = FALSE) {

  if (missing(x = mainDir)) {
    stop("Must provide a working directory and sample names.")
  } else if(!is.numeric(consDepth) | consDepth < 0){
    stop("consDepth needs to be a positive integer.")
  } else if(!dir.exists(mainDir)) {
    stop("You must supply a valid directory.")
  } else if(!is.null(sampleNames) && !is.list(sampleNames)){
    warning("sampleNames must be NULL or list. Using defaults.")
    sampleNames = NULL
  }

  # Get sample names
  if (is.null(sampleNames)){
    dir.names <- list.dirs(
      path = mainDir,
      full.names = FALSE,
      recursive = FALSE
    )
  }

  seq.Data <- tibble()

  if(length(dir.names) == 0){
    stop("No sample folders found. Have you supplied a valid top level
         directory containing umierrorcorrect output folders?")
  }

  for (i in 1:length(dir.names)) {

    seq.Table <- readBamFile(
      sampleDir = file.path(mainDir, dir.names[i]),
      consDepth = consDepth
    )

    # If NULL is returned by readBamFile no bam file was found
    if(is.null(seq.Table)){
      print(
        paste(
          "No bam file was found for sample:",dir.names[i],
          "in directory:",mainDir,
          sep=" "
        )
      )
    }

    # If running in shiny app, displat a loading bar
    if(as.shiny){
      n <- length(dir.names)
      shiny::incProgress(1/n, detail = paste("Parsing reads", i, " of ", n))
    }

    seq.Table$sample <- dir.names[i]

    seq.Data <- dplyr::bind_rows(seq.Data, seq.Table)
  }

  return(seq.Data)
}

#' Find consensus reads
#' A function to analyse consensus read tables generated with parseBamFiles or
#' a UMIexperiment object containing reads.
#'
#' @export
#'
#' @import tibble
#'
#' @param object Either a tibble generated with parseBamFiles or a UMIexperiment object
#' @param pattern Regular expression
#' @param consDepth Minimum consensus depth to keep. Default is 0.
#' @param groupBy Should data be grouped by position, sample, both or not at all.
#'
findConsensusReads <- function(
  object,
  consDepth = 0,
  groupBy = c("none", "sample", "position", "both"),
  pattern = NULL) {

  if(missing(x = object)){
    stop("No object supplied")
  } else if(!tibble::is_tibble(object) || !class(object) == "UMIexperiment") {
    stop("Need to supply either a UMIexperiment object tibble generated
         with parseBamFiles.")
  }

  if (class(object)[1] == "UMIexperiment") {
    readsTable <- object@reads
  } else {
    readsTable <- object
  }
}

#' Method for filtering UMIexperiment and sample objects
#' @export
#'
#' @importFrom magrittr "%>%" "%<>%"
#' @importFrom utils data
#' @importFrom dplyr filter
#' @importFrom tibble as_tibble
#'
#' @param object Requires a UMI sample or UMI experiment object.
#' @param name String. Name of the filter. Default is "default".
#' @param minDepth Consensus depth to analyze. Default is 3.
#' @param minCoverage Mininum coverage required for amplicons. Default is 1.
#' @param minFreq Minimum variant allele frequency to keep. Default is 0.
#' @param minCount Minimum variant allele count to keep. Default is 3.
#'
#' @return A UMI sample or UMI experiment object.
#'
#'
filterUmiObject <- function(
  object,
  name = "default",
  minDepth = 3,
  minCoverage = 100,
  minFreq = 0,
  minCount = 0) {

  if (missing(x = object)) {
    stop("Must provide a umiExperiment object and filter name.")
  } else if(!class(object) == "UMIexperiment"){
    stop("Object is not of class UMIexperiment.")
  } else if(minDepth < 3){
    warning("You set minDepth to a value below 3. This will severely impact
            error correction.")
  } else if(minCoverage < 100){
    warning("Minimum coverage is below 50 consensus reads. Data with so few
            reads may be very unreliable.")
  } else if( tibble::is_tibble(object@filters[name][[1]]) ){
    warning("Filter ", name, " already exists. Will be overwritten.")
  }

  cons.table <- object@cons.data

  raw.error <- cons.table %>%
    dplyr::filter(
      .data$`Consensus group size` == 0,
      .data$Coverage >= minCoverage,
      .data$Name != '',
      .data$`Max Non-ref Allele Frequency` >= minFreq,
      .data$`Max Non-ref Allele Count` >= minCount
    )

  cons.table <- cons.table %>%
    dplyr::filter(
      .data$`Consensus group size` == minDepth,
      .data$Coverage >= minCoverage,
      .data$Name != '',
      .data$`Max Non-ref Allele Frequency` >= minFreq,
      .data$`Max Non-ref Allele Count` >= minCount
    )

  object@filters[[name]] <- cons.table
  object@raw.error <- raw.error

  return(object)
}

#' Method for retrieving filtered data
#' @export
#' @importFrom readr write_excel_csv write_delim
#' @param object Requires a UMI sample or UMI experiment object.
#' @param name String. Name of the filter. Default is "default".
#' @param save Logical, should data be saved as csv file? Default is FALSE.
#' @param outDir Output directory
#' @param fileName Filename to be used, default is the same as 'name'
#' @param delim Character string denoting delimiter to be used, default is ';'.
#' @return A filtered consensus table, as a tibble.
#'
getFilteredData <- function(
  object,
  name = 'default',
  save = FALSE,
  outDir = getwd(),
  fileName = NULL,
  delim = ';'
  ) {

  if (missing(x = object)) {
    stop("Must provide a umiExperiment object and filter name.")
  } else if(!class(object) == "UMIexperiment"){
    stop("Object is not of class UMIexperiment.")
  } else if(!is.logical(save)){
    warning("save needs to be of type boolean. Using defaults instead.")
    save = FALSE
  } else if(!dir.exists(outDir)){
    warning("outDir needs to be a valid path. Using working directory.")
    outDir = getwd()
  } else if(!is.character(fileName) && !is.null(fileName)){
    stop("fileName needs to be a string or NULL")
  } else if(! delim %in% c(';',',','\t')){
    warning("Invalid delimeter, needs to be comma, semicolon or tab.
            Using comma instead.")
    delim = ','
  } else if(is.null(object@filters[name][[1]])) {
    if(!is.null(object@filters$default)){
      warning("Requested filter not found, using default.")
      name = 'default'
    } else {
      stop("Filter not found. Have you run filterUmiObject?")
    }
  }

  filter <- object@filters[name][[1]]

  if (is.null(fileName)) {
    outFile <- paste(name, ".csv", sep = '')
  } else {
    outFile <- paste(fileName, ".csv", sep = '')
  }

  if (save) {
    path <- file.path(outDir, outFile)
    if (delim == ';') {
      readr::write_excel_csv(filter, path, delim = ';')
    } else if (delim == ',') {
      readr::write_excel_csv(filter, path)
    } else {
      readr::write_delim(filter, path, delim = delim)
    }
  } else {
    return(filter)
  }
}


#' Beta distribution
#'
#' Calculates the negative log likelihood for the beta distribution.
#'
#' @param params Non-negative parameters of the Beta distribution.
#' @param data consensus.data table of a UMisample or UMIexperiment object.
#'
#' @importFrom stats dbeta
#'
#' @return Negative log-likelihood for beta distribution.
#'
betaNLL <- function(params, data) {
  a <- params[1]
  b <- params[2]

  # negative log likelihood for beta
  return(-sum(stats::dbeta(data, shape1 = a, shape2 = b, log = TRUE)))
}

#' callVariants using beta binomial distribution
#'
#' Calculate variant p-values using permutation-based testing. A prior is fitted
#' to model the background error using maximum likelihood estimation of a beta
#' distribution. The maximum likelihood estimate of the beta distribution is then
#' used to define the shape of a beta-binomial distribution used to estimate
#' variant P-Values. This can be interpreted as a probability for a variant to
#' not have arisen by chance.
#'
#' @export
#'
#' @param object A UMIerrorcorrect object.
#' @param minDepth Minimum consensus depth required fedault is 3
#' @param minCoverage Minimum Coverage to use, default is 100 reads.
#' @param computePrior Should a new distribution be derived from data? Default is FALSE.
#'
#' @importFrom dplyr mutate progress_estimated
#' @importFrom tibble as_tibble
#' @importFrom stats nlm var p.adjust
#' @importFrom utils install.packages
#' @importFrom graphics plot
#'
#' @return Object containing raw and FDR-adjusted P-Values
#' @seealso \code{\link{filterVariants}} on how to filter variants.
#'
#'
callVariants <- function(
  object,
  minDepth = 3,
  minCoverage = 100,
  computePrior = FALSE
  ) {

  if (missing(x = object)) {
    stop("Must provide a umiExperiment object.")
  } else if(!class(object) == "UMIexperiment"){
    stop("Object is not of class UMIexperiment.")
  } else if(!is.numeric(minDepth) || minDepth < 3){
    if(minDepth < 0){
      stop("minDepth must be a positive integer.")
    } else if (minDepth < 3){
      warning("Mindepth is smaller than 3, this may affect error correction.")
    }
  } else if(!is.numeric(minCoverage) || minCoverage < 100){
    if(minCoverage < 0){
      stop("minCoverage must be a positive integer.")
    } else if(minCoverage < 100){
      warning("minCoverage is smaller than 100, this may affect error correction.")
    }
  }

  object <- filterUmiObject(
    object = object,
    name = "varCalls",
    minDepth = minDepth,       # Require minDepth
    minCoverage = minCoverage, # Require at least minCoverage cons reads
    minFreq = 0,               # no minimum allele freq
    minCount = 0              # no minimum variant allele count
  )

  cons.table <- object@filters["varCalls"][[1]]

  a1 <- cons.table$Coverage * cons.table$`Max Non-ref Allele Frequency` # No. of variant alleles
  b1 <- cons.table$Coverage # Total coverage

  m <- mean(a1 / b1) # average background count
  v <- var(a1 / b1) # variance of background counts

  # Calculate initial values
  a0 <- m * (m * (1 - m) / v - 1)
  b0 <- (1 - m) * (m * (1 - m) / v - 1)
  params0 <- c(a0, b0)

  # If computePrior == TRUE fit a new beta binomial background distribution,
  # otherwise use pre-computed values (default)
  if(computePrior){
    fit <- stats::nlm(betaNLL, params0, a0 / b0)
    a <- fit$estimate[1]
    b <- fit$estimate[2]
    pval <- NULL

  } else {
    a <- 2.168215069116764
    b <- 3531.588541594945
    pval <- NULL
  }

  for (i in 1:length(a1)) { # for each named amplicon position:
    #r1 <- VGAM::rbetabinom.ab(10000, b1[i], shape1 = a, shape2 = b) # Calculate probability of success
    r1 <- beta_binom(10000, b1[i], shape1 = a, shape2 = b)
    pval[i] <- sum(r1 > a1[i]) / 10000 # Estimate p value of variant
  }

  padj <- stats::p.adjust(p = pval, method = 'fdr')

  cons.table <- dplyr::mutate(cons.table, pval = pval)
  cons.table <- dplyr::mutate(cons.table, p.adjust = padj)

  object@variants <- cons.table

  object <- addMetaData(
    object = object,
    attributeName = "varCalls", "varCalls"
  )

  return(object)
}


#' Filter variants based on p values or depth
#'
#' You can filter variants called with the the "callVariants" function based
#' on adjusted p-value, minimum variant allele count and supply a list
#' of assays and samples to plot.
#'
#' @export
#' @importFrom magrittr "%>%" "%<>%"
#' @importFrom dplyr select filter between
#' @importFrom tibble as_tibble
#' @importFrom rlang .data
#' @param object A UMIexperiment object
#' @param p.adjust Numeric. Adjusted p value (FDR). Default is 0.2.
#' @param minVarCount Integer. Minimum variant allele count. Default is 5.
#' @param amplicons NULL or list of assays to plot. NULL uses all.
#' @param samples NULL or list of samples to plot. NULL uses all.
#' @seealso \code{\link{callVariants}} on how to call variants.
#' @return A UMIexperiment object with filtered variants. Can be used to
#'   generate vcf files.
#'
filterVariants <- function(
  object,
  p.adjust = 0.2,
  minVarCount = 5,
  amplicons = NULL,
  samples = NULL
  ) {

  if (missing(x = object)) {
    stop("Must provide a umiExperiment object.")
  } else if(!class(object) == "UMIexperiment"){
    stop("Object is not of class UMIexperiment.")
  } else if(!dplyr::between(p.adjust, 0, 1)) {
    warning("Adjusted p-value cutoff needs to be between 0 and 1, using defaults.")
    p.adjust = 0.2
  } else if(minVarCount < 0) {
    warning("minVarCount must be a positive integer. Using defaults instead.")
    minVarCount = 5
  }

  # TODO update the check for presence of the variant data to checking object@variants instead of attributes

  if ("varCalls" %in% names(attributes(object))) {
    # Load the consensus data from object
    vars.to.print <- object@variants

    # Filter based on p-value and minimum variant allele depth and select important columns
    # using .data also prevents R CMD check from giving a NOTE about undefined global variables
    # (provided that you have also imported rlang::.data with @importFrom rlang .data).

    vars.to.print <- filterConsensusTable(
      consensus.data = vars.to.print,
      amplicons =  amplicons,
      samples = samples
    )

    vars.to.print <- vars.to.print %>%
      dplyr::filter(
        .data$`Max Non-ref Allele Count` >= minVarCount,
        .data$p.adjust <= p.adjust
      ) %>%
      dplyr::select(
        .data$`Sample Name`,
        .data$Contig,
        .data$Position,
        .data$Name,
        .data$Reference,
        .data$`Max Non-ref Allele`,
        .data$p.adjust,
        .data$Coverage,
        .data$`Max Non-ref Allele Count`,
        .data$`Max Non-ref Allele Frequency`,
        .data$sample
      )

    print(vars.to.print)
    object@variants <- vars.to.print

    return(object)
  }
  else {
    stop("You need to run callVariants before running filterVariants.")
  }
}

#' Import experimental design meta data such as replicates, treatments, categorical variables.
#' @export
#' @importFrom utils read.table
#' @param object UMI.experiment to which to add metadata
#' @param file File containing meta data
#' @param delim Column separator. Default is NULL (automatically determine delimiter)
#'
importDesign <- function(
  object,
  file,
  delim = NULL
  ){

  # Error Handling
  if (missing(x = object) || missing(x = file)) {
    stop("Must provide a umiExperiment object and file name.")
  } else if(!class(object) == "UMIexperiment"){
    stop("Object is not of class UMIexperiment.")
  } else if(!is.character(file)) {
    stop("File must be a valid name.")
  }

  if(is.null(delim)) {
    # Automatically determine file type if delim = NULL
    # Import data using all three delimiters and then check the number of
    # columns. If the delimiter is wrong, there will only be one column.

    # TODO this works for these delimiters but doesn't handle other
    # delimiters well.
    comma <- read.table(file = file, sep = ',', header = TRUE)
    semicolon <- read.table(file = file, sep = ';', header = TRUE)
    tab <- read.table(file = file, sep = '\t', header = TRUE)

    if(ncol(comma) > 1){
      metaData <- comma
      print("Uploading comma separated meta data file.")
    } else if(ncol(semicolon) > 1) {
      metaData <- semicolon
      print("Uploading semicolon separated meta data file.")
    } else if(ncol(tab) > 1) {
      metaData <- tab
      print("Uploading tab separated meta data file.")
    } else {
      warning('Automatic delimiter selection failed: It seems like your metadata
              file is not delimited by either comma, semicolon or tab.')
    }
  } else if (!delim %in% c(',', ';', '\t')) {
    # If delim is not NULL and not comma, simicolon or tab, throw exception
    stop("Delimiter needs to be one of: c(',', ';', '\t')")
  } else {
    # Import file using user defined delimiter
    metaData <- read.table(
      file = file,
      sep = delim,
      header = TRUE
    )
  }

  # Add imported table to the object meta.data slot
  object@meta.data <- metaData
  object <- addMetaData(object = object, attributeName = 'design', metaData)

  return(object)
}

#' mergeTechnicalReplicates
#'
#' A function to merge replicates in UMIexperiment object. This will result in a merged data set
#' accessible from the UMIexperiment object using merged.data. This is meant to provide statistical
#' information across multiple replicates. If you want to merge multiple sequencing runs of the
#' sample into a single sample using the collapseReplicates function instead.
#'
#' @param object UMI.experiment to which to add metadata
#' @param filter.name Name of the filter to use. Defaults to "default".
#' @param do.plot Should normalization plot be shown. Default is TRUE.
#' @param group.by Variable used to group data. If NULL sample names will be used.
#' @param amplicons List of amplicons to use
#' @param samples List of samples to use
#' @param normalise.by.sample If TRUE, normalises reads depth by both samples and assays. Otherwise only assays are used.
#' @param remove.singletons Remove variants only found in one replicate.
#' @param zero.counts Number between 0 and 1. What values should negative counts get?
#' @param option Colour sclae for plotting.
#' @param direction Direction of colour scale if using viridis package.
#'
#' @export
#'
#' @import dplyr
#' @importFrom magrittr "%>%" "%<>%"
#' @importFrom rlang .data
#' @importFrom stats sd
#'
#'
#' @return A umiExperiment object
mergeTechnicalReplicates <- function(
  object,
  filter.name = 'default',
  do.plot = TRUE,
  group.by = NULL,
  amplicons = NULL,
  samples = NULL,
  normalise.by.sample = FALSE,
  remove.singletons = TRUE,
  zero.counts = 0.5,
  option = c('viridis', 'magma', 'plasma', 'inferno'),
  direction = c(1, -1)
  ) {

  # Error handling
  if (missing(x = object)) {
    stop("Must provide a umiExperiment object.")
  } else if(!class(object) == "UMIexperiment"){
    stop("Object is not of class UMIexperiment.")
  } else if(is.null(object@filters[filter.name][[1]])) {
    if(!is.null(object@filters$default)){
      warning("Requested filter ", filter.name, " not found, using default.")
      filter.name = 'default'
    } else {
      stop("Filter not found. Have you run filterUmiObject?")
    }
  } else if(!is.logical(do.plot)) {
    warning("do.plot needs to  be of type boolean. Using default instead.")
    do.plot = TRUE
  }

  # Import filtered data
  consData <- getFilteredData(
    object = object,
    name = filter.name
  )

  consData$Position %<>% as.factor

  # Select amplicons and samples
  consData <- filterConsensusTable(
    consData,
    amplicons = amplicons,
    samples = samples
  )

  # Get metadata and
  metaData <- as_tibble(object@meta.data)
  if (is.null(group.by)){
    metaData$group.by <- dplyr::pull(metaData, 1)
    metaData$group.by %<>% as.factor
  } else {
    metaData$group.by <- dplyr::pull(metaData, group.by)
    metaData$group.by %<>% as.factor
  }

  # Join meta data and consData replicate ID column to consData
  # Change to left_join?
  # First column of meta data needs to contain sample names
  consData <- dplyr::inner_join(consData, metaData, by = c(`Sample Name` = colnames(metaData[,1]) ))

  # Calculate normalization factor
  consData <- consData %>% group_by(.data$Name) %>%
    mutate(normFac= mean(.data$Coverage)/.data$Coverage) %>% # Normalization factor
    mutate(normCoverage = .data$Coverage*.data$normFac)  %>% # Normalized Coverage
    ungroup()

  # Plot coverage before and after normalization
  plot.norm <- vizNormalization(consData)

  # Summarize normalized data for output
  consData <- consData %>%
    # Group data by factors
    dplyr::group_by(
      .data$Name,
      .data$Contig,
      .data$Position,
      .data$Reference,
      .data$group.by
    ) %>%
    dplyr::summarise(
      avg.A = mean(.data$A*.data$normFac),
      avg.T = mean(.data$T*.data$normFac),
      avg.C = mean(.data$C*.data$normFac),
      avg.G = mean(.data$G*.data$normFac),
      avg.N = mean(.data$N*.data$normFac),
      avg.I = mean(.data$I*.data$normFac),
      avg.D = mean(.data$D*.data$normFac),
      avg.Depth = mean(.data$Coverage*.data$normFac),
      std.Depth = sd(.data$Coverage*.data$normFac),
      avg.Max.AF = mean(.data$`Max Non-ref Allele Frequency`),
      std.MaxAF = sd(.data$`Max Non-ref Allele Frequency`),
      avg.MaxAC = mean(.data$`Max Non-ref Allele Count`),
      std.MaxAC = sd(.data$`Max Non-ref Allele Count`)
    )

  # Plot normalised counts stacked by variant allele
  stacked.counts <- vizStackedCounts(
    consData,
    option = option,
    direction = direction
  )

  if(do.plot){
    # Return object
    object@plots$stacked_counts <- stacked.counts
    object@plots$norm_plot <- plot.norm
    object@merged.data <- consData

    print(object@plots$stacked_counts)

    return(object)

  } else {
    # Return object
    object@plots$stacked_counts <- stacked.counts
    object@plots$norm_plot <- plot.norm
    object@merged.data <- consData
    return(object)
  }
}
